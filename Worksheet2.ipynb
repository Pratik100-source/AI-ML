{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOb40maxoZnEDuZxN6b2HRC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"KTyNOtcIn6ff","executionInfo":{"status":"ok","timestamp":1741236691950,"user_tz":-345,"elapsed":1653,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}}},"outputs":[],"source":["# Necessary Imports\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","source":["df = pd.read_excel(\"/content/Iris.csv (1).xlsx\") # changed to read_csv and the correct file name\n","# Step 2: Dataset Information\n","print(\"Dataset Preview:\")\n","print(df.head())  # Show first 5 rows\n","print(\"\\nDataset Information:\")\n","print(df.info())  # Summary of dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBgoyxMIuBbu","executionInfo":{"status":"ok","timestamp":1741236705087,"user_tz":-345,"elapsed":881,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}},"outputId":"a7efcb8a-ff60-4402-97ba-e5d9c4a278cb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset Preview:\n","   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n","0   1            5.1           3.5            1.4           0.2  Iris-setosa\n","1   2            4.9           3.0            1.4           0.2  Iris-setosa\n","2   3            4.7           3.2            1.3           0.2  Iris-setosa\n","3   4            4.6           3.1            1.5           0.2  Iris-setosa\n","4   5            5.0           3.6            1.4           0.2  Iris-setosa\n","\n","Dataset Information:\n","<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 150 entries, 0 to 149\n","Data columns (total 6 columns):\n"," #   Column         Non-Null Count  Dtype  \n","---  ------         --------------  -----  \n"," 0   Id             150 non-null    int64  \n"," 1   SepalLengthCm  150 non-null    float64\n"," 2   SepalWidthCm   150 non-null    float64\n"," 3   PetalLengthCm  150 non-null    float64\n"," 4   PetalWidthCm   150 non-null    float64\n"," 5   Species        150 non-null    object \n","dtypes: float64(4), int64(1), object(1)\n","memory usage: 7.2+ KB\n","None\n"]}]},{"cell_type":"code","source":["# Step 3: Extract features (X) and target labels (y)\n","X = df.iloc[:, 1:-1].values  # All columns except the first and the last one (features) since the first column is an index\n","y = df.iloc[:, -1].values   # Last column (target)\n","\n","# Step 4: Convert categorical labels to numeric\n","label_encoder = LabelEncoder()\n","y_encoded = label_encoder.fit_transform(y)  # Convert labels to integers (0,1,2)\n","\n","# Step 5: One-Hot Encode the Labels\n","one_hot_encoder = OneHotEncoder(sparse_output=False) #changed sparse to sparse_output and set to False\n","y_one_hot = one_hot_encoder.fit_transform(y_encoded.reshape(-1, 1))\n","\n","# Display results\n","print(\"\\nUnique Classes:\", np.unique(y))\n","print(\"Encoded Labels:\", np.unique(y_encoded))\n","print(\"One-Hot Encoded Labels:\\n\", y_one_hot[:5])  # Show first 5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3ulYtUVYy6LD","executionInfo":{"status":"ok","timestamp":1741236723002,"user_tz":-345,"elapsed":29,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}},"outputId":"d3e3aa05-371c-4012-a144-78c89de4c08e"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Unique Classes: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n","Encoded Labels: [0 1 2]\n","One-Hot Encoded Labels:\n"," [[1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]\n"," [1. 0. 0.]]\n"]}]},{"cell_type":"code","source":["# Step 6: Split dataset into training (80%) and testing (20%) sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y_one_hot, test_size=0.2, random_state=42, stratify=y_one_hot)\n","\n","# Output shapes\n","print(\"\\nShapes:\")\n","print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n","print(\"X_test:\", X_test.shape, \"y_test:\", y_test.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2qZn2KcDzAP2","executionInfo":{"status":"ok","timestamp":1741236798194,"user_tz":-345,"elapsed":54,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}},"outputId":"9962bfb5-5a2b-4862-a966-a265f9545b66"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Shapes:\n","X_train: (120, 4) y_train: (120, 3)\n","X_test: (30, 4) y_test: (30, 3)\n"]}]},{"cell_type":"markdown","source":["## Decision Function or Model."],"metadata":{"id":"YmSZUGKImFC7"}},{"cell_type":"code","source":["import numpy as np\n","\n","def softmax(z):\n","    \"\"\"\n","    Compute the softmax probabilities for a given input matrix.\n","\n","    Parameters:\n","    z (numpy.ndarray): Logits (raw scores) of shape (m, n), where\n","                       - m is the number of samples.\n","                       - n is the number of classes.\n","\n","    Returns:\n","    numpy.ndarray: Softmax probability matrix of shape (m, n), where\n","                   each row sums to 1 and represents the probability\n","                   distribution over classes.\n","\n","    Notes:\n","    - The input to softmax is typically computed as: z = XW + b.\n","    - Uses numerical stabilization by subtracting the max value per row.\n","    \"\"\"\n","\n","    # Prevent numerical instability by normalizing input\n","    z_shifted = z - np.max(z, axis=1, keepdims=True)\n","    exp_z = np.exp(z_shifted)\n","    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n"],"metadata":{"id":"Ayz7aD4ZzQta","executionInfo":{"status":"ok","timestamp":1741236841159,"user_tz":-345,"elapsed":65,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# This test case checks that each row in the resulting softmax probabilities sums to 1, which is the\n","# Example test case\n","\n","z_test = np.array([[2.0, 1.0, 0.1], [1.0, 1.0, 1.0]])\n","softmax_output = softmax(z_test)\n","# Verify if the sum of probabilities for each row is 1 using assert\n","row_sums = np.sum(softmax_output, axis=1)\n","# Assert that the sum of each row is 1\n","assert np.allclose(row_sums, 1), f\"Test failed: Row sums are {row_sums}\"\n","print(\"Softmax function passed the test case!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dflVaZRF0179","executionInfo":{"status":"ok","timestamp":1741237591659,"user_tz":-345,"elapsed":13,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}},"outputId":"b71ae1ab-bc41-4533-f41a-df357b71dce0"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Softmax function passed the test case!\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","def loss_softmax(y_pred, y):\n","    \"\"\"\n","    Compute the cross-entropy loss.\n","\n","    Parameters:\n","    y_pred (numpy.ndarray): Predicted probabilities of shape (n, c), where n is the number of samples and c is the number of classes.\n","    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c).\n","\n","    Returns:\n","    float: Cross-entropy loss.\n","    \"\"\"\n","    epsilon = 1e-12  # To avoid log(0)\n","    y_pred = np.clip(y_pred, epsilon, 1.0 - epsilon)  # Prevent log(0) by clipping values\n","    n = y.shape[0]  # Number of samples\n","    loss = -np.sum(y * np.log(y_pred)) / n\n","    return loss"],"metadata":{"id":"hWE1V5nG2zoV","executionInfo":{"status":"ok","timestamp":1741237754045,"user_tz":-345,"elapsed":39,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def cost_softmax(X, y, W, b):\n","    \"\"\"\n","    Compute the softmax regression cost (cross-entropy loss).\n","\n","    Parameters:\n","    X (numpy.ndarray): Feature matrix of shape (n, d), where n is the number of samples and d is the number of features.\n","    y (numpy.ndarray): True labels (one-hot encoded) of shape (n, c), where c is the number of classes.\n","    W (numpy.ndarray): Weight matrix of shape (d, c).\n","    b (numpy.ndarray): Bias vector of shape (c,).\n","\n","    Returns:\n","    float: The softmax cost (cross-entropy loss).\n","    \"\"\"\n","    n = X.shape[0]  # Number of samples\n","    z = np.dot(X, W) + b\n","    y_pred = softmax(z)\n","    cost = loss_softmax(y_pred, y)\n","    return cost"],"metadata":{"id":"9Y7BwK9k28RA","executionInfo":{"status":"ok","timestamp":1741237767493,"user_tz":-345,"elapsed":6,"user":{"displayName":"Pratik Panthi","userId":"05570757109847948890"}}},"execution_count":12,"outputs":[]}]}